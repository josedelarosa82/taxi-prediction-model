{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ¿Qué problema se va a resolver?\n",
        "El tema de investigación surgió por la necesidad que tienen las empresas operadoras de taxis de la ciudad de Bogotá, estas vienen presentando una creciente problemática de indisponibilidad de servicios de taxis en sus plataformas, esto suceso ocurre principalmente en algunas zonas u horarios específicos.\n",
        "La empresa Taxis Libres (TL), una de las más grandes operadoras de servicios de taxis de la ciudad, ha empezado a recibir una gran cantidad de inconformidades por partes de los usuarios recurrentes de la plataforma por esta misma causa. Solo en el mes de mayo del 2022 la plataforma recibió alrededor de 1.2 millones de solicitudes de servicios de taxis de los cuales el 57% de esas solicitudes fueron abandonadas o rechazadas, lo que los llevó a analizar qué estaba pasando con la prestación de servicios internamente, en esa revisión realizada durante el mismo periodo de tiempo se obtuvieron las principales causas reportadas en la plataforma y que se listarán a continuación:\n",
        "\n",
        "•\tEl usuario se fue\n",
        "\n",
        "•\tEl vehículo se demoró en llegar\n",
        "\n",
        "•\tEl usuario tomó otro taxi\n",
        "\n",
        "Al identificar las causas de los servicios abandonados y rechazados, la empresa se planteó a resolver la siguiente pregunta al problema presentado.\n",
        "¿Como reducir la alta cantidad de inconformidades de los usuarios recurrentes de la plataforma prediciendo sus comportamientos durante la semana y así programarles sus servicios de forma anticipada?\n",
        "\n",
        "## Objetivo general\n",
        "Diseñar un método para predecir las solicitudes de servicios de los usuarios recurrentes \"de empresas de servicios de\" taxis en la ciudad Bogotá - Colombia, \"mediante\" modelos de ML.\n",
        "\n",
        "\n",
        "Evaluar el desempeño de modelos de ML para predecir\n",
        "las solicitudes de servicios de los usuarios recurrentes de empresas de servicios de taxis en la ciudad Bogotá -  Colombia\n",
        "basados en el comportamiento dentro de la plataforma.\n",
        "\n",
        "## Objetivos específicos (Detallar las actividades)\n",
        "• Revisar la literatura de al menos 10 artículos de proyectos relacionados (pendiente)\n",
        "\n",
        "• Analizar los datos de los últimos 6 meses de servicios de la empresa taxis libres.\n",
        "\n",
        "• Determinar las variables significativas a ser empleadas en los modelos de ML.\n",
        "\n",
        "• Evaluar diferentes modelos de ML para la predicción de solicitudes.\n",
        "\n",
        "## ¿Que solución propone al problema?\n",
        "El diseño de un modelo predictivo de solicitudes de servicios de taxis basados en comportamientos de usuarios recurrentes de plataformas móviles para empresas de transporte\n",
        "\n",
        "## ¿Cómo lo pretende solucionar?\n",
        "Utilizando un modelo de clasificación donde se agrupe la información de los servicios que han tomado los usuarios por días de las semanas y horarios, determinando si en ese día y a esa hora se va a tomar un servicio.\n",
        "\n",
        "## ¿Que resultados que espera obtener?\n",
        "Se espera obtener listado por día de las semanas con todos los horarios del día y un determinador que indique si el servicio se va a tomar o no en ese horario. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm7tiHiJk80E"
      },
      "source": [
        "### Exploratory data analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project library definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YlqkYPGeDuz"
      },
      "outputs": [],
      "source": [
        "#librerias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from matplotlib.colors import ListedColormap\n",
        "#from mlxtend.plotting import plot_decision_regions\n",
        "#modelos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "#from sklearn.metrics import mean_absolute_error\n",
        "#from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "import numpy as np\n",
        "#from numpy import NaN\n",
        "from datetime import datetime, timedelta\n",
        "#from os import name\n",
        "import math"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yj6GLCBW1jan"
      },
      "source": [
        "The initial parameters are defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLoQZVku1iMb"
      },
      "outputs": [],
      "source": [
        "TRAIN_WEEK = 8.0\n",
        "TEST_WEEK = 1.0\n",
        "BOGOTA_CODE = 11001\n",
        "HOURS_OF_DAY = 24\n",
        "WEEKS_RECURRENT = 6\n",
        "FORMAT_DATE = '%Y-%m-%d'\n",
        "FORMAT_COMPLETE_DATE = '%Y-%m-%d %H:%M:%S'\n",
        "DATE_START_TRAINIG = pd.to_datetime(\"2023-03-10\", format=FORMAT_DATE)\n",
        "FICHERO_DATA = '../data/SERVICIO_UNIFICADO_2023.parquet.gzip'\n",
        "\n",
        "#Se obtiene la fecha de inicial de entrenamiento\n",
        "last_date_dataset = DATE_START_TRAINIG - timedelta(weeks=TRAIN_WEEK)\n",
        "#Se obtiene la fecha de inicial de entrenamiento\n",
        "first_date_dataset = DATE_START_TRAINIG + timedelta(weeks=TEST_WEEK)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is loaded into the dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnMbR7vKA1KO",
        "outputId": "d19dfb19-63d8-4113-dd65-f304764645d8"
      },
      "outputs": [],
      "source": [
        "#Se lee el archivo de un parquet a un dataframe\n",
        "df_taxis = pd.read_parquet(FICHERO_DATA) \n",
        "#Se visualizan los datos\n",
        "print(f\"({len(df_taxis):,}) records were loaded into dataset!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data types of the dataset are displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validates how many null values are present in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data is cleaned where the user is null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis = df_taxis[~df_taxis['USER'].isnull()]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset fields are displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.head(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display of initial data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services in a period of time is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.groupby(['DATE'])['ID'].count().plot(kind='line',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services per state per month is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.groupby(['MONTH','STATUS'])['ID'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display of filtered data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is filtered with the values to be used: 6 weeks for training and 1 week for the test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis = df_taxis[ (pd.to_datetime(df_taxis['COMPLETEDATE'], format=FORMAT_DATE) >= last_date_dataset) & (pd.to_datetime(df_taxis['COMPLETEDATE'], format=FORMAT_DATE) <= first_date_dataset) ]\n",
        "df_taxis = df_taxis.sort_values(by=['COMPLETEDATE','USER'])\n",
        "print(f\"The dataset is restricted from date:{last_date_dataset} - to date:{first_date_dataset}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The categorical data of channel is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se visualiza la cantidad por origen\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.histplot(df_taxis['CHANNEL'])\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The categorical data of status is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se visualiza la cantidad de servicios por estado\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.histplot(df_taxis['STATUS'])\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DataFrame data is displayed in a histogram to analyze the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.hist(figsize=(20,10))\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services per channel per state is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.groupby(['CHANNEL','STATUS'])['ID'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: delete this block\n",
        "df_taxis.groupby(['DATE','DAYOFWEEK'])['USER'].count()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Channels are limited to: APP, RECEPCION, CHAT_BOT_659, IVR and CALLE\n",
        "\n",
        "- States are limited to: ABANDONADO, CUMPLIDO, CANCELADO and FINALIZADO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis = df_taxis[(df_taxis[\"CHANNEL\"]==\"APP\") ]#| (df_taxis[\"CHANNEL\"]==\"RECEPCION\") | (df_taxis[\"CHANNEL\"]==\"CHAT_BOT_659\") | (df_taxis[\"CHANNEL\"]==\"IVR\") | (df_taxis[\"CHANNEL\"]==\"CALLE\")]\n",
        "df_taxis = df_taxis[(df_taxis[\"STATUS\"]==\"ABANDONADO\") | (df_taxis[\"STATUS\"]==\"CUMPLIDO\") | (df_taxis[\"STATUS\"]==\"CANCELADO\") | (df_taxis[\"STATUS\"]==\"FINALIZADO\")]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlations between variables are displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(df_taxis.corr(method='pearson'),annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A histogram is displayed with the number of services per state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis.groupby(['MONTH','STATUS'])['ID'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=0)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services per status is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "s_taxis = df_taxis.groupby(['STATUS','HOUR'])['DATE'].count()\n",
        "s_taxis.plot(ax=ax)\n",
        "_ = ax.set(\n",
        "    title=\"Number of services per status\",\n",
        "    xticks=[i * 24 for i in range(4)],\n",
        "    xticklabels=[\"ABANDONADO\", \"CANCELADO\", \"CUMPLIDO\", \"FINALIZADO\"],\n",
        "    xlabel=\"Status per hour\",\n",
        "    ylabel=\"Amount of services\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Columns that are not useful for the model are deleted and reindex the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis = df_taxis.drop(columns=[\"CITY\",\"ID\",\"LATITUDEDEST\",\"LONGITUDEDEST\",\"LATITUDEORI\",\"LONGITUDEORI\",\"COMPLETEDATE\",\"CHANNEL\",\"STATUS\",\"MONTH\"])\n",
        "#Duplicate values are removed from the dataframe and sorted by the most relevant fields\n",
        "df_taxis = df_taxis.drop_duplicates().sort_values(by=['DATE','USER','DAYOFWEEK','HOUR'])\n",
        "#The data frame is reindexed again\n",
        "df_taxis = df_taxis.reset_index(drop=True)\n",
        "#The data frame is displayed\n",
        "df_taxis.head(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se eliminan los usuarios no recurrentes del dataset (Usuarios recurrentes: son aquellos que tomaron al menos 1 servicio semanal durante almenos 4 semanas anteriores)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services per week is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "s_taxis = df_taxis.groupby(['DAYOFWEEK','HOUR'])['DATE'].count()\n",
        "s_taxis.plot(ax=ax)\n",
        "_ = ax.set(\n",
        "    title=\"Number of services per week\",\n",
        "    xticks=[i * 24 for i in range(7)],\n",
        "    xticklabels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thr\", \"Fri\", \"Sat\"],\n",
        "    xlabel=\"Day of week\",\n",
        "    ylabel=\"Amount of services\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services per hour is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "s_taxis = df_taxis.groupby(['HOUR'])['DATE'].count()\n",
        "s_taxis.plot(ax=ax)\n",
        "_ = ax.set(\n",
        "    title=\"Number of services per hour\",\n",
        "    xticks=[i for i in range(24)],\n",
        "    xticklabels=[i for i in range(24)],\n",
        "    xlabel=\"Hour of day\",\n",
        "    ylabel=\"Amount of services\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of services assigned to a user is counted after eliminating duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_services = df_taxis.loc[:,['USER']].value_counts()\n",
        "count_services"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of the data is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se visualizan los valores de los datos\n",
        "count_services.describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of outliers in the dataset is calculated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#The 25th quartile is calculated\n",
        "q25 = count_services.quantile(0.25)\n",
        "#The 50th quartile is calculated\n",
        "q50 = count_services.quantile(0.5)\n",
        "#The 75th quartile is calculated\n",
        "q75 = count_services.quantile(0.75)\n",
        "#The  max value is calculated\n",
        "max_val = count_services.max()\n",
        "#The inter-quartile range is calculated\n",
        "iqr = q75 - q25\n",
        "\n",
        "print(f\"El porcentaje del quartil 25: {q25}\")\n",
        "print(f\"El porcentaje del quartil 75: {q75}\")\n",
        "print(f\"El porcentaje del máximo: {max_val}\")\n",
        "print(f\"El rango inter quartil es: {iqr}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculation of the outlier threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outliers_threshold = ( q75 + 3 * iqr )\n",
        "print(f\"El umbral de los valores atípicos es: {outliers_threshold}\")\n",
        "print(f\"La cantidad de valores atípicos son: {sum(count_services > outliers_threshold) }\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outliers are removed from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_taxis = df_taxis.groupby('USER').filter(lambda x: (x['USER'].count() > outliers_threshold) )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se Validan los datos del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_services = df_taxis.loc[:,['USER']].value_counts()\n",
        "count_services"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DATA VISUALIZATION\n",
        "\n",
        "* Gráfico de dispersión de las variables\n",
        "* Gráfico de distribución de las variables (Baja distribución, Baja predicción)\n",
        "* Gráficas de correlación (Mapa de calor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se presenta un gráfico general de los datos\n",
        "sns.pairplot(df_taxis, height=4)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The correlation matrix is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix = df_taxis.corr()\n",
        "corr_matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The relationship between variables in the data are displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 8))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(corr_matrix, annot=True, mask = mask, cmap=cmap )\n",
        "plt.title('Relationship between dataframe variables',fontsize=20,fontname='serif')\n",
        "plt.ylabel('Y', color='green', fontsize=25)\n",
        "plt.xlabel('X', color='Red', fontsize=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#limites de decision\n",
        "def make_meshgrid(X, step=.02):\n",
        "    x = X.iloc[:,0].values\n",
        "    y = X.iloc[:,1].values\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
        "    return xx, yy\n",
        "\n",
        "def plot_contours(ax, clf, X, y, **params):\n",
        "\n",
        "    markers = ('o', 's', '^', 'v', '<')\n",
        "    colors = ('red', 'blue', 'cyan', 'gray')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    xx, yy = make_meshgrid(X)\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "\n",
        "    # plot class examples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        Z[Z == cl] = idx\n",
        "        ax.scatter(X.loc[y == cl, X.columns[0]], \n",
        "                    X.loc[y == cl, X.columns[1]],\n",
        "                    alpha=0.8, \n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl, \n",
        "                    edgecolor='black',\n",
        "                    cmap=cmap,\n",
        "                    s=50)\n",
        "    \n",
        "    ax.legend()\n",
        "    ax.set_ylabel(X.columns[1])\n",
        "    ax.set_xlabel(X.columns[0])\n",
        "    ax.set_title(clf.__class__.__name__)\n",
        "    ax.grid()    \n",
        "    out = ax.contourf(xx, yy, Z, alpha=0.2, cmap=cmap)\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ROC Y AUC\n",
        "def ROC(X, y, model):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "  clf = OneVsRestClassifier(model)\n",
        "  clf.fit(X_train, y_train)\n",
        "  pred = clf.predict(X_test)\n",
        "  pred_prob = clf.predict_proba(X_test)\n",
        "  pred_prob = np.nan_to_num(pred_prob)\n",
        "\n",
        "  fpr = {}\n",
        "  tpr = {}\n",
        "  thresh ={}\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,8))\n",
        "  colors = ('orange', 'green', 'blue')\n",
        "  \n",
        "  for index, label_ in enumerate(y.unique()):\n",
        "    fpr[label_], tpr[label_], thresh[label_] = roc_curve(y_test, pred_prob[:,index], pos_label=label_) \n",
        "    ax.plot(fpr[label_], tpr[label_], linestyle='--',color=colors[index], label=label_)\n",
        "  \n",
        "  ax.plot([0, 1], [0, 1], linestyle='--',color='red', label='No Skill')\n",
        "  ax.set_title(f'Multiclass ROC curve -  {model.__class__.__name__}')\n",
        "  ax.set_xlabel('False Positive Rate')\n",
        "  ax.set_ylabel('True Positive rate')\n",
        "  ax.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  return ax.legend(loc='best')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to train the model and predict with test values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predictModel(model, X_train, y_train, X_test):\n",
        "    #A continuación se entrena el modelo\n",
        "    model.fit(X_train, y_train)\n",
        "    #Se crean las predicciones para pruebas\n",
        "    y_pred = model.predict(X_test)\n",
        "    #Se crean las probabilidades para pruebas\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "    return model, y_pred, y_prob"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the K-neighbors model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knnModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de K vecinos\n",
        "    model = KNeighborsClassifier(n_neighbors=2)#, metric='euclidean',leaf_size=1, p=1, weights='uniform')\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Multi-layer Perceptron model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mlpModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de perceptrones multi capa\n",
        "    model = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(30,30, 30), random_state=1,learning_rate_init=0.001,max_iter=5000)\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Perceptron model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de perceptrones multi capa\n",
        "    model = Perceptron(eta0=0.1, n_iter_no_change=10, random_state=1)\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lrModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de de regesión logística\n",
        "    model = LogisticRegression(random_state = 1)\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rfcModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de los arboles aleatorios\n",
        "    model = RandomForestClassifier(criterion='gini', max_depth=5, n_estimators=200, class_weight=\"balanced\")\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Support Vector Machine model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def svcModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de máquina de soporte de vectores\n",
        "    model = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0, class_weight='balanced')#, class_weight='balanced'\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Decision Tree model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dtcModel(X_train, y_train, X_test):\n",
        "    #Se aplica el modelo de los arboles de desiciones\n",
        "    model = DecisionTreeClassifier(criterion='gini', max_depth=3, class_weight='balanced')#class_weight={0: 1, 1: 5}\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate Extra Trees Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def etcModel(X_train, y_train, X_test):\n",
        "    model = ExtraTreesClassifier(n_estimators=1, max_depth=1, min_samples_split=2, random_state=0, class_weight=\"balanced\")\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate Gradient Boosting Classifier, this function supports both binary and multi-class classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gbcModel(X_train, y_train, X_test):\n",
        "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate Ada Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def abcModel(X_train, y_train, X_test):\n",
        "    model = AdaBoostClassifier(n_estimators=1,algorithm='SAMME',random_state=0)\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that evaluate the Stochastic Gradient Descent model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sgdModel(X_train, y_train, X_test,):\n",
        "    #Se aplica el modelo de gradiente descendente estocástico\n",
        "    model =SGDClassifier(loss='huber', random_state=1, max_iter=2000, epsilon=0.1)#loss='hinge'\n",
        "    return predictModel(model, X_train, y_train, X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function for displaying data visualization with subplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataVisualization(df_train):\n",
        "    #Se visualiza los datos por columnas de entrenamiento\n",
        "    fig, axes = plt.subplots(1, len(df_train.columns))\n",
        "    fig.set_size_inches(21,6)\n",
        "    for i, column in enumerate(df_train.columns):\n",
        "        sns.histplot(df_train[column], kde=True, ax=axes[i])\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to display the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confusionMatrix(model, y_test, y_pred):\n",
        "  '''\n",
        "  plt.rcParams.update({'font.size': 16})\n",
        "  fig, axes = plt.subplots(figsize=(14, 10))\n",
        "  disp = ConfusionMatrixDisplay.from_estimator(model,\n",
        "                                             y_test,\n",
        "                                             y_pred,\n",
        "                                             display_labels=model.classes_,\n",
        "                                             cmap=plt.cm.Blues,\n",
        "                                             ax=axes\n",
        "                                             )\n",
        "  disp.ax_.set_title(f'Confusion Matrix - {model.__class__.__name__}')\n",
        "  plt.show()\n",
        "  '''\n",
        "  plt.rcParams.update({'font.size': 16})\n",
        "  cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                display_labels=model.classes_)\n",
        "  disp.plot()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contoursGrap(model, X, y):\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    fig, axes = plt.subplots(figsize=(12, 10))\n",
        "    plot_contours(axes, model, X, y, cmap=plt.cm.coolwarm, alpha=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to display the different reports for comparing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type):\n",
        "    if model_type == 'LR':\n",
        "        #Se imprimen los coeficientes, intresección y número de coeficientes\n",
        "        print(f\"Coeficientes del modelo: {model.coef_}\\n\")\n",
        "        print(f\"Intresección del modelo: {model.intercept_}\\n\")\n",
        "        print(f\"Número de coeficientes del modelo: {len(model.coef_)}\\n\")\n",
        "    if model_type != 'PR':\n",
        "\n",
        "        if (model_type != 'SVC') & (model_type != 'SGD'):\n",
        "            #\n",
        "            ROC(X_train, y_train, model)\n",
        "        #\n",
        "        contoursGrap(model, X_train, y_train)\n",
        "        #\n",
        "    #Se muestra la matriz de confusión\n",
        "    confusionMatrix(model, y_test, y_pred)\n",
        "    #Se imprime el reporte de clasificación\n",
        "    #print(classification_report(y_test, y_pred))\n",
        "    #mae = mean_absolute_error(y_test, y_pred)\n",
        "    #print(\"MAE:\",(mae))\n",
        "    \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to calculate a user's probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ProbabilisticClassifier(hour, day, data):\n",
        "  data_array = data.to_numpy()\n",
        "  first = pd.to_datetime(data.tail(1).iloc[0,2], format=FORMAT_DATE)\n",
        "  last = pd.to_datetime(data.head(1).iloc[0,2], format=FORMAT_DATE)\n",
        "  remaining_days = first.date() - last.date()\n",
        "  total = 0 \n",
        "  for i in range(remaining_days.days):\n",
        "    last_time = last + timedelta(days=i)\n",
        "    if last_time.isoweekday() == day:\n",
        "      total += 1\n",
        "  total_onset = 0\n",
        "  for row in data_array:\n",
        "    #print(f\" day({row[0]} == {day}) hour({row[1]} == {hour}) service({row[3]} == {1})\")\n",
        "    if( ( row[0] == day ) & ( row[1] == hour ) & ( row[3] == 1 ) ):\n",
        "      total_onset += 1\n",
        "  if total > 0:\n",
        "    return total_onset / total\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to evaluate the probabilistic model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def probabilisticModel(data, date_predict):\n",
        "    threshold = 0.7\n",
        "    array_result = []\n",
        "    array_prob = []\n",
        "    for hour in range(HOURS_OF_DAY):\n",
        "        result = ProbabilisticClassifier(hour, date_predict.isoweekday(), data)\n",
        "        array_prob.append(round(result,2) if result >= threshold else 0 )\n",
        "        array_result.append(1 if result >= threshold else 0)\n",
        "    return array_result, array_prob"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to identify how many weeks a user took services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weeksWithServices(user, data, training_date):\n",
        "    weeks = 0\n",
        "    date_time = training_date\n",
        "    for _ in range(int(TRAIN_WEEK)):\n",
        "      first_time = pd.to_datetime(date_time, format=FORMAT_DATE)\n",
        "      last_time = first_time + timedelta(weeks=-1)\n",
        "      df_rec = data[ (data[\"USER\"]==user) & (pd.to_datetime(data['DATE'], format=FORMAT_DATE) >= last_time) & (pd.to_datetime(data['DATE'], format=FORMAT_DATE) <= first_time) ]\n",
        "      date_time = last_time\n",
        "      weeks += 1 if len(df_rec) > 0 else 0\n",
        "    return weeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recurrentPerHour(p_user, df_tx, training_date):\n",
        "    date_time = training_date\n",
        "    current_week = training_date.isoweekday()\n",
        "    first_time = pd.to_datetime(date_time, format=FORMAT_DATE)\n",
        "    last_time = first_time + timedelta(weeks=-TRAIN_WEEK)\n",
        "    for hour in range(HOURS_OF_DAY):\n",
        "      df_rec = df_tx[ (df_tx[\"USER\"] == p_user) & (df_tx[\"DAYOFWEEK\"] == current_week) & (df_tx['DATE'].astype(str) >= last_time.strftime(FORMAT_DATE)) & (df_tx['DATE'].astype(str) <= first_time.strftime(FORMAT_DATE)) & (df_tx['HOUR'] == hour) ]\n",
        "      total = len(df_rec)\n",
        "      if total >= WEEKS_RECURRENT:\n",
        "         return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recurrentPerDay(p_user, df_tx, training_date):\n",
        "    weeks_count = 0\n",
        "    date_time = training_date\n",
        "    current_week = training_date.isoweekday()\n",
        "    for _ in range(int(TRAIN_WEEK)):\n",
        "      first_time = pd.to_datetime(date_time, format=FORMAT_DATE)\n",
        "      last_time = first_time + timedelta(weeks=-1)\n",
        "      df_rec = df_tx[ (df_tx[\"USER\"] == p_user) & (df_tx[\"DAYOFWEEK\"] == current_week) & (df_tx['DATE'].astype(str) >= last_time.strftime(FORMAT_DATE)) & (df_tx['DATE'].astype(str) <= first_time.strftime(FORMAT_DATE)) ]\n",
        "      date_time = last_time\n",
        "      weeks_count += 1 if len(df_rec) > 0 else 0\n",
        "      if weeks_count >= WEEKS_RECURRENT:\n",
        "         return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Función para validar si contiene todos los datos en 0\n",
        "def existAllZero(arr):\n",
        "    if len(arr)>0:\n",
        "        for value in arr:\n",
        "            if value != 0:\n",
        "                return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Conversor del codigo a nombre de modelos\n",
        "def modelName(model_type):\n",
        "    if model_type == 'LR':\n",
        "        return 'Logistic regression'\n",
        "    elif model_type == 'KNN':\n",
        "        return 'Kneighbors'\n",
        "    elif model_type == 'MLP':\n",
        "        return 'Multilayer perceptron'\n",
        "    elif model_type == 'RFC':\n",
        "        return 'Random forest'\n",
        "    elif model_type == 'SVC':\n",
        "        return 'Support vector'\n",
        "    elif model_type == 'SGD':\n",
        "        return 'Gradient descent stochastic'\n",
        "    elif model_type == 'DTC':\n",
        "        return 'Decision tree'\n",
        "    elif model_type == 'GBC':\n",
        "        return 'Gradient boosting'\n",
        "    elif model_type == 'ETC':\n",
        "        return 'Extra trees'\n",
        "    elif model_type == 'ABC':\n",
        "        return 'Ada Boost'\n",
        "    elif model_type == 'PR':\n",
        "        return 'Probabilistic'\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Funcion que permite rellenar el dataset con los servicios tomados en 1 y los no tomados en 0\n",
        "def setDataService(data, from_date, to_date):\n",
        "  remaining_days = to_date.date() - from_date.date()\n",
        "  service_array = []\n",
        "  existService = 0\n",
        "  for i in range(remaining_days.days+1):\n",
        "    last_time = from_date + timedelta(days=i)\n",
        "    if last_time.date().isoweekday() == to_date.date().isoweekday():\n",
        "      for hour in range(HOURS_OF_DAY):\n",
        "        #Se filtra por el mismo día de la semana, fecha y hora\n",
        "        #df_service = data[(data['DIADESEMANA'] == last_time.isoweekday()) & (data['FECHA'] == last_time.date()) & (data['HORA'] == hour) ].to_numpy()\n",
        "        df_service = data[ (data['DATE'].astype(str) == last_time.strftime(FORMAT_DATE)) & (data['HOUR'] == hour) ]\n",
        "        if len(df_service) > 0:\n",
        "          #print(f\"fecha con servicio ={last_time.strftime(FORMAT_DATE)}, hour ={hour}\")\n",
        "          #Se llena con valor en 1 porqué se encontró un servicio\n",
        "          existService = 1\n",
        "        else:\n",
        "          #print(f\"fecha sin servicio ={last_time.strftime(FORMAT_DATE)}, hour ={hour}\")\n",
        "          #Se llena con valor en 0 porqué no se encontró ningún servicio\n",
        "          existService = 0        \n",
        "        df_service = [last_time.date().strftime(FORMAT_DATE),last_time.isoweekday(),hour,existService]\n",
        "        service_array.append(df_service)\n",
        "  return pd.DataFrame(service_array, \n",
        "             columns=['DATE','DAYOFWEEK','HOUR','SERVICE'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to test the different models in a configured time range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Función que entrene el modelo y nos devuelva el y_predicted\n",
        "def evaluate_model(phone, data, date_predict, model_type):\n",
        "  #Inicialización de vectores\n",
        "  y_pred = np.empty(0,dtype=int)\n",
        "  y_prob = np.empty(0,dtype=float)\n",
        "  y_test = np.empty(0,dtype=int)\n",
        "\n",
        "  y_result_arr = []\n",
        "  y_prob_arr = []\n",
        "  #Se filtra por el número de teléfono\n",
        "  data = data[data[\"USER\"]==phone].copy()\n",
        "  #Se elimina el teléfono debido a que no es relevante para el modelo\n",
        "  data = data.drop(labels=['USER'], axis=1)\n",
        "  \n",
        "  #Se obtiene la fecha de inicial de entrenamiento\n",
        "  date_first_training = date_predict - timedelta(weeks=TRAIN_WEEK)\n",
        "  \n",
        "  #Se particiona el dataframe desde la fecha inicial y hasta la fecha final de entrenamiento\n",
        "  data = data[ (pd.to_datetime(data['DATE'], format=FORMAT_DATE) >= date_first_training) & (pd.to_datetime(data['DATE'], format=FORMAT_DATE) <= date_predict) ]\n",
        "  \n",
        "  #Se obtiene el tamaño de los datos del usuario\n",
        "  tam = len(data)\n",
        "\n",
        "  #Se valida si tiene más de un servicio registrado\n",
        "  if tam > 1:\n",
        "    #Se llenan en 0 las horas de los servicios que no se tomaron por el usuario\n",
        "    data = setDataService(data, date_first_training, date_predict)\n",
        "    #Se crear el dataframe de entrenamiento desde la fecha inicial hasta la fecha de predicción\n",
        "    train = data[ (pd.to_datetime(data['DATE'], format=FORMAT_DATE) >= date_first_training) & (pd.to_datetime(data['DATE'], format=FORMAT_DATE) < date_predict) ]\n",
        "    #Se crear el dataframe de pruebas desde la fecha de predicción hasta una semana adelante\n",
        "    test = data[(pd.to_datetime(data['DATE'], format=FORMAT_DATE) == date_predict) ]\n",
        "\n",
        "\n",
        "    #Datos para probar los modelos\n",
        "    if model_type != 'PR':\n",
        "      train = train.loc[:,['DAYOFWEEK','HOUR','SERVICE']]\n",
        "      test = test.loc[:,['DAYOFWEEK','HOUR','SERVICE']]\n",
        "    else:\n",
        "      train = train.loc[:,['DAYOFWEEK','HOUR','DATE','SERVICE']]\n",
        "      test = test.loc[:,['DAYOFWEEK','HOUR','SERVICE']]\n",
        "\n",
        "    y_train = train.SERVICE\n",
        "    X_train = train.drop(labels='SERVICE', axis=1)\n",
        "\n",
        "    y_test = test.SERVICE\n",
        "    X_test = test.drop(labels='SERVICE', axis=1)\n",
        "\n",
        "    \n",
        "    if model_type != 'PR':\n",
        "      '''\n",
        "      scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "      #scaler = StandardScaler()\n",
        "      #scaler = MinMaxScaler(feature_range=(0,1))\n",
        "      #scaler = Normalizer()\n",
        "      scaler.fit(X_train)\n",
        "      X_train = scaler.transform(X_train)\n",
        "      scaler.fit(X_test)\n",
        "      X_test = scaler.transform(X_test)\n",
        "      '''\n",
        "    \n",
        "    if model_type == 'LR':\n",
        "      #Se visualiza los datos\n",
        "      #dataVisualization(train)\n",
        "      model, y_pred, y_prob = lrModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'KNN':\n",
        "      model, y_pred, y_prob = knnModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'MLP':\n",
        "      model, y_pred, y_prob = mlpModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'RFC':\n",
        "      model, y_pred, y_prob = rfcModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'SVC':\n",
        "      model, y_pred, y_prob = svcModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'SGD':\n",
        "      model, y_pred, y_prob = sgdModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'DTC':\n",
        "      model, y_pred, y_prob = dtcModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'GBC':\n",
        "      model, y_pred, y_prob = gbcModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'ETC':\n",
        "      model, y_pred, y_prob = etcModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'ABC':\n",
        "      model, y_pred, y_prob = abcModel(X_train, y_train, X_test)\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    elif model_type == 'PR':\n",
        "      #Se caulcula el modelo probabilistico\n",
        "      y_result, y_proba = probabilisticModel(train, date_predict)\n",
        "      \n",
        "      y_result_arr.append(y_result)\n",
        "      y_prob_arr.append(y_proba)\n",
        "      \n",
        "      for i in y_result_arr:\n",
        "        for j in i:\n",
        "          y_pred = np.append(y_pred, int(j))\n",
        "      for i in y_prob_arr:\n",
        "        for j in i:\n",
        "          y_prob = np.append(y_prob, float(j))\n",
        "      class model:\n",
        "        classes_ = np.empty(0,dtype=int)\n",
        "        name = 'Probabilistic'\n",
        "      model.classes_ = np.array([0, 1])\n",
        "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
        "    else:\n",
        "      print(\"Modelo no existe!\")\n",
        "    #else:\n",
        "    #  print(f\"No hay servicios suficientes para la fecha:{date_predict} del usuario:{phone}!\")\n",
        "  return y_pred, y_test, y_prob"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A new dataFrame is created with the telephone numbers to go through the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_user, count, user_array = 100000, 0, []\n",
        "df_user = df_taxis.loc[:,['USER']].copy()\n",
        "series_user = df_user.loc[:,['USER']].value_counts()\n",
        "for user in series_user.index.to_list():\n",
        "    if count < total_user:\n",
        "        #tot = weeksWithServices(user[0], df_taxis, DATE_START_TRAINIG)\n",
        "        #if tot < WEEKS_RECURRENT:\n",
        "            #print(f\"The user is deleted {user[0]} because user isn't recurrent\")    \n",
        "        #df_taxis.drop(df_taxis[df_taxis['USER'] == user[0]].index, inplace = True)\n",
        "        #else:\n",
        "        count += 1\n",
        "        user_array.append(user[0])\n",
        "    else:\n",
        "        break\n",
        "print(f\"Total users: {len(user_array)}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment with all models for one week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se quitan los modelos ,'LR','SVC','SGD','MLP' por mal rendimiento\n",
        "models_array = ['PR','KNN','ETC','ABC','GBC','RFC','DTC']#,\n",
        "#models_array = ['PR']\n",
        "report_models = []\n",
        "roc_curve_arr = []\n",
        "start_proccess = datetime.now()\n",
        "print(f\"Start process: {datetime.now()}\")\n",
        "for model in models_array:\n",
        "    start_date = datetime.now()\n",
        "    count = 0\n",
        "    kappa_score = 0.0\n",
        "    test_score = 0\n",
        "    roc_score = 0.0\n",
        "    confusion_phone = np.matrix(0,dtype=int)\n",
        "    y_test_arr = np.array(0,dtype=int)\n",
        "    y_prob_arr = np.array(0,dtype=float)\n",
        "    for user in user_array:\n",
        "        remaining_days = DATE_START_TRAINIG.date() + timedelta(weeks=TEST_WEEK) - DATE_START_TRAINIG.date()\n",
        "        for i in range(remaining_days.days):\n",
        "            last_time = DATE_START_TRAINIG.date() + timedelta(days=i)\n",
        "            #Se valida que en al semana exista almenos 4 servicios con el día\n",
        "            if recurrentPerHour(user, df_taxis, pd.to_datetime(last_time, format=FORMAT_DATE)):\n",
        "                y_pred, y_test, y_prob = evaluate_model(user, df_taxis, pd.to_datetime(last_time, format=FORMAT_DATE), model)\n",
        "                #if(not existAllZero(y_pred)) & (not existAllZero(y_test)):\n",
        "                if len(y_prob)>0:\n",
        "                    y_test = y_test.values[:]\n",
        "                    y_prob = [y_prob[:, 1] if model != 'PR' else y_prob[:] ]\n",
        "                    y_test_arr = np.concatenate((y_test_arr, y_test), axis=None)\n",
        "                    y_prob_arr = np.concatenate((y_prob_arr, y_prob), axis=None)\n",
        "                if len(y_pred) > 1:\n",
        "                    try:\n",
        "                        matrix = np.matrix(confusion_matrix(y_test, y_pred))\n",
        "                        if matrix.size == 1:\n",
        "                            matrix = np.matrix([[24,0],[0,0]],dtype=int)\n",
        "                        confusion_phone = confusion_phone + matrix\n",
        "                    except Exception as err:\n",
        "                        #print(f\"Error in confusion_matrix: {err}\")\n",
        "                        confusion_phone = confusion_phone\n",
        "                    try:\n",
        "                        score_k = cohen_kappa_score(y_test, y_pred)\n",
        "                        kappa_score += (0.0 if math.isnan (score_k) else score_k)\n",
        "                    except Exception as err:\n",
        "                        #print(f\"Error in kappa_score: {err}\")\n",
        "                        kappa_score += 0\n",
        "                    test_score += np.mean(y_pred == y_test)\n",
        "                    try:\n",
        "                        roc_score += roc_auc_score(y_test, y_pred)\n",
        "                    except Exception as err:\n",
        "                        #print(f\"Error in roc_score: {err}\")\n",
        "                        roc_score += 0.0\n",
        "                    count = count + 1\n",
        "                #else:\n",
        "                #    print(f\"The user: {phone} didn't take services on {last_time}\")\n",
        "    roc_curve_arr.append([model, y_test_arr, y_prob_arr])\n",
        "    report_models.append([model,confusion_phone, (kappa_score/count) if kappa_score > 0 else 0, (test_score/count) if test_score > 0 else 0, (roc_score/count) if roc_score > 0 else 0])\n",
        "    print(f\"Processed {modelName(model)} model in: {datetime.now()-start_date}\")\n",
        "print(f\"Finish process: {datetime.now()} in: {datetime.now()-start_proccess}\")\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se ejecuta la matriz de confusión de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_array = []\n",
        "for model in report_models:\n",
        "    true_neg = model[1][0,0]\n",
        "    true_pos = model[1][1,1]\n",
        "    false_pos = model[1][0,1]\n",
        "    false_neg = model[1][1,0]\n",
        "    kappa = model[2]\n",
        "    test_score = model[3]\n",
        "    roc = model[4]\n",
        "\n",
        "    #Precision = TruePositives / (TruePositives + FalsePositives)\n",
        "    precision_scr = true_pos / (true_pos + false_pos) if true_pos > 0 else 0\n",
        "    #Recall = TruePositives / (TruePositives + FalseNegatives)\n",
        "\n",
        "    \n",
        "    recall_scr = true_pos / (true_pos + false_neg) if true_pos > 0 else 0\n",
        "    #F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
        "    f1_scr = (2 * precision_scr * recall_scr) / (precision_scr + recall_scr) if true_pos > 0 else 0\n",
        "\n",
        "    #Sacar la tasa de los TP = Que es Positivos del test sobre los positivos predichos\n",
        "    #TP rate = TP / Positivos reales (test)\n",
        "    \n",
        "    #FP rate = FP / Negativos reales (test)\n",
        "\n",
        "    models_array.append([modelName(model[0]),\n",
        "                         round(precision_scr,2),\n",
        "                         round(recall_scr,2),\n",
        "                         round(f1_scr,2),\n",
        "                         true_neg,\n",
        "                         true_pos,\n",
        "                         false_pos,\n",
        "                         false_neg,\n",
        "                         round(kappa,2),\n",
        "                         round(test_score,2),\n",
        "                         round(roc,2)])\n",
        "\n",
        "#Mostramos el dataframe con el reporte\n",
        "pd.DataFrame(models_array, columns = ['MODEL','PRECISION','RECALL','F1','TN','TP','FP','FN','KAPPA','SCORE TEST','ROC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se muestra la curva ROC de los modelos\n",
        "for roc in roc_curve_arr:\n",
        "    if not existAllZero(roc[1]):\n",
        "        ns_probs = [0 for _ in range(len(roc[1]))]\n",
        "        # calculate scores\n",
        "        ns_auc = roc_auc_score(roc[1], ns_probs)\n",
        "        lr_auc = roc_auc_score(roc[1], roc[2])\n",
        "        # summarize scores\n",
        "        \n",
        "        print(f'{modelName(roc[0])}: ROC AUC=%.3f' % (lr_auc))\n",
        "        # calculate roc curves\n",
        "        ns_fpr, ns_tpr, _ = roc_curve(roc[1], ns_probs)\n",
        "        lr_fpr, lr_tpr, _ = roc_curve(roc[1], roc[2])\n",
        "        # plot the roc curve for the model\n",
        "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No model')\n",
        "        plt.plot(lr_fpr, lr_tpr, marker='.', label=modelName(roc[0]))\n",
        "        # axis labels\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        # show the legend\n",
        "        plt.legend()\n",
        "        # show the plot\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"El modelo {modelName(roc[0])} no tiene valores para mostrar\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Proyecto predicción servicio taxi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c66cf74b0d2a9f1f4e9764d44e7ed6418e0349c942d329ab910649654051b15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
