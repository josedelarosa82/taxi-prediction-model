{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué problema se va a resolver?\n",
    "El tema de investigación surgió por la necesidad que tienen las empresas operadoras de taxis de la ciudad de Bogotá, estas vienen presentando una creciente problemática de indisponibilidad de servicios de taxis en sus plataformas, esto suceso ocurre principalmente en algunas zonas u horarios específicos.\n",
    "La empresa Taxis Libres (TL), una de las más grandes operadoras de servicios de taxis de la ciudad, ha empezado a recibir una gran cantidad de inconformidades por partes de los usuarios recurrentes de la plataforma por esta misma causa. Solo en el mes de mayo del 2022 la plataforma recibió alrededor de 1.2 millones de solicitudes de servicios de taxis de los cuales el 57% de esas solicitudes fueron abandonadas o rechazadas, lo que los llevó a analizar qué estaba pasando con la prestación de servicios internamente, en esa revisión realizada durante el mismo periodo de tiempo se obtuvieron las principales causas reportadas en la plataforma y que se listarán a continuación:\n",
    "\n",
    "•\tEl usuario se fue\n",
    "\n",
    "•\tEl vehículo se demoró en llegar\n",
    "\n",
    "•\tEl usuario tomó otro taxi\n",
    "\n",
    "Al identificar las causas de los servicios abandonados y rechazados, la empresa se planteó a resolver la siguiente pregunta al problema presentado.\n",
    "¿Como reducir la alta cantidad de inconformidades de los usuarios recurrentes de la plataforma prediciendo sus comportamientos durante la semana y así programarles sus servicios de forma anticipada?\n",
    "\n",
    "## Objetivo general\n",
    "Diseñar un método para predecir las solicitudes de servicios de los usuarios recurrentes \"de empresas de servicios de\" taxis en la ciudad Bogotá - Colombia, \"mediante\" modelos de ML.\n",
    "\n",
    "\n",
    "Evaluar el desempeño de modelos de ML para predecir\n",
    "las solicitudes de servicios de los usuarios recurrentes de empresas de servicios de taxis en la ciudad Bogotá -  Colombia\n",
    "basados en el comportamiento dentro de la plataforma.\n",
    "\n",
    "## Objetivos específicos (Detallar las actividades)\n",
    "• Revisar la literatura de al menos 10 artículos de proyectos relacionados (pendiente)\n",
    "\n",
    "• Analizar los datos de los últimos 6 meses de servicios de la empresa taxis libres.\n",
    "\n",
    "• Determinar las variables significativas a ser empleadas en los modelos de ML.\n",
    "\n",
    "• Evaluar diferentes modelos de ML para la predicción de solicitudes.\n",
    "\n",
    "## ¿Que solución propone al problema?\n",
    "El diseño de un modelo predictivo de solicitudes de servicios de taxis basados en comportamientos de usuarios recurrentes de plataformas móviles para empresas de transporte\n",
    "\n",
    "## ¿Cómo lo pretende solucionar?\n",
    "Utilizando un modelo de clasificación donde se agrupe la información de los servicios que han tomado los usuarios por días de las semanas y horarios, determinando si en ese día y a esa hora se va a tomar un servicio.\n",
    "\n",
    "## ¿Que resultados que espera obtener?\n",
    "Se espera obtener listado por día de las semanas con todos los horarios del día y un determinador que indique si el servicio se va a tomar o no en ese horario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm7tiHiJk80E"
   },
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project library definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting holidays\n",
      "  Downloading holidays-0.27.1-py3-none-any.whl (598 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.5/598.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from holidays) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil->holidays) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YlqkYPGeDuz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#librerias\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "#from mlxtend.plotting import plot_decision_regions\n",
    "#modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "#from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import numpy as np\n",
    "#from numpy import NaN\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "#from os import name\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj6GLCBW1jan"
   },
   "source": [
    "The initial parameters are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLoQZVku1iMb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_WEEK = 6.0\n",
    "TEST_WEEK = 1.0\n",
    "BOGOTA_CODE = 11001\n",
    "HOURS_OF_DAY = 24\n",
    "WEEKS_RECURRENT = 5\n",
    "FORMAT_DATE = '%Y-%m-%d'\n",
    "FORMAT_COMPLETE_DATE = '%Y-%m-%d %H:%M:%S'\n",
    "DATE_START_TRAINIG = pd.to_datetime(\"2023-03-10\", format=FORMAT_DATE)\n",
    "FICHERO_DATA = 's3://taxis-ml-2023-05-27/data/SERVICIO_UNIFICADO_2023.parquet.gzip'\n",
    "\n",
    "#Se obtiene la fecha de inicial de entrenamiento\n",
    "last_date_dataset = DATE_START_TRAINIG - timedelta(weeks=TRAIN_WEEK)\n",
    "#Se obtiene la fecha de inicial de entrenamiento\n",
    "first_date_dataset = DATE_START_TRAINIG + timedelta(weeks=TEST_WEEK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded into the dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnMbR7vKA1KO",
    "outputId": "d19dfb19-63d8-4113-dd65-f304764645d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se lee el archivo de un parquet a un dataframe\n",
    "df_taxis = pd.read_parquet(FICHERO_DATA) \n",
    "#Se visualizan los datos\n",
    "print(f\"({len(df_taxis):,}) records were loaded into dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data types of the dataset are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validates how many null values are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is cleaned where the user is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis = df_taxis[~df_taxis['USER'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset fields are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display of initial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of services in a period of time is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.groupby(['DATE'])['ID'].count().plot(kind='line',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of services per state per month is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.groupby(['MONTH','STATUS'])['ID'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: delete this block\n",
    "df_taxis.groupby(['DATE','DAYOFWEEK'])['USER'].count().plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Date, Day of week\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: delete this block\n",
    "df_taxis.groupby(['MONTH','DAYOFWEEK'])['USER'].count().plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month, Day of week\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis[(df_taxis['STATUS']=='ABANDONADO') | (df_taxis['STATUS']=='CUMPLIDO') | (df_taxis['STATUS']=='CANCELADO')].groupby(['STATUS'])['USER'].count().plot(kind='pie',stacked=True, ylabel=\"STATUS\", fontsize=14, autopct='%1.1f%%', radius=1.5, shadow=True, figsize=(12,8), rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: delete this block\n",
    "df_taxis.groupby(['MONTH','STATUS'])['USER'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: delete this block\n",
    "df_taxis[df_taxis['STATUS']=='ABANDONADO'].groupby(['MONTH'])['USER'].count().plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services per month\", rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: delete this block\n",
    "df_taxis.groupby(['MONTH'])['USER'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Display of filtered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is filtered with the values to be used: 6 weeks for training and 1 week for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis = df_taxis[ (pd.to_datetime(df_taxis['COMPLETEDATE'], format=FORMAT_COMPLETE_DATE) >= last_date_dataset) & (pd.to_datetime(df_taxis['COMPLETEDATE'], format=FORMAT_COMPLETE_DATE) <= first_date_dataset) ]\n",
    "df_taxis = df_taxis.sort_values(by=['COMPLETEDATE','USER'])\n",
    "print(f\"The dataset is restricted from date:{last_date_dataset} - to date:{first_date_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical data of channel is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se visualiza la cantidad por origen\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df_taxis['CHANNEL'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical data of status is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se visualiza la cantidad de servicios por estado\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df_taxis['STATUS'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame data is displayed in a histogram to analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.hist(figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of services per channel per state is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.groupby(['CHANNEL','STATUS'])['ID'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: delete this block\n",
    "df_taxis.groupby(['DATE','DAYOFWEEK'])['USER'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Channels are limited to: APP, RECEPCION, CHAT_BOT_659, IVR and CALLE\n",
    "\n",
    "- States are limited to: ABANDONADO, CUMPLIDO, CANCELADO and FINALIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis = df_taxis[(df_taxis[\"CHANNEL\"]==\"APP\") ]#| (df_taxis[\"CHANNEL\"]==\"RECEPCION\") | (df_taxis[\"CHANNEL\"]==\"CHAT_BOT_659\") | (df_taxis[\"CHANNEL\"]==\"IVR\") | (df_taxis[\"CHANNEL\"]==\"CALLE\")]\n",
    "df_taxis = df_taxis[(df_taxis[\"STATUS\"]==\"ABANDONADO\") | (df_taxis[\"STATUS\"]==\"CUMPLIDO\") | (df_taxis[\"STATUS\"]==\"CANCELADO\") | (df_taxis[\"STATUS\"]==\"FINALIZADO\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.drop(labels='STATUS', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between variables are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_taxis.loc[:,['LATITUDEORI','LATITUDEDEST','LONGITUDEORI','LONGITUDEDEST','DAYOFWEEK','HOUR','MONTH']].corr(method='pearson'),annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram is displayed with the number of services per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis.groupby(['MONTH','STATUS'])['ID'].count().unstack('STATUS').plot(kind='bar',stacked=True, fontsize=14, xlabel=\"Month\", ylabel=\"Amount of services\", figsize=(12,8), title=\"Services by period\", rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of services per status is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "s_taxis = df_taxis.groupby(['STATUS','HOUR'])['DATE'].count()\n",
    "s_taxis.plot(ax=ax)\n",
    "_ = ax.set(\n",
    "    title=\"Number of services per status\",\n",
    "    xticks=[i * 24 for i in range(4)],\n",
    "    xticklabels=[\"ABANDONADO\", \"CANCELADO\", \"CUMPLIDO\", \"FINALIZADO\"],\n",
    "    xlabel=\"Status per hour\",\n",
    "    ylabel=\"Amount of services\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that are not useful for the model are deleted and reindex the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis = df_taxis.drop(columns=[\"CITY\",\"ID\",\"LATITUDEDEST\",\"LONGITUDEDEST\",\"LATITUDEORI\",\"LONGITUDEORI\",\"COMPLETEDATE\",\"CHANNEL\",\"STATUS\",\"MONTH\"])\n",
    "#Duplicate values are removed from the dataframe and sorted by the most relevant fields\n",
    "df_taxis = df_taxis.drop_duplicates().sort_values(by=['DATE','USER','DAYOFWEEK','HOUR'])\n",
    "#The data frame is reindexed again\n",
    "df_taxis = df_taxis.reset_index(drop=True)\n",
    "#The data frame is displayed\n",
    "df_taxis.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-recurrent users are removed from the dataset (Recurrent users are those who took at least 6 services on the same day and at the same time during the last 8 previous weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recurrentUsers(p_user, df_tx, training_date):\n",
    "    current_week = training_date.isoweekday()\n",
    "    first_time = pd.to_datetime(training_date, format=FORMAT_DATE)\n",
    "    last_time = first_time + timedelta(weeks=-TRAIN_WEEK)\n",
    "    for hour in range(HOURS_OF_DAY):\n",
    "      df_rec = df_tx[ (df_tx[\"USER\"] == p_user) & (df_tx[\"DAYOFWEEK\"] == current_week) & (df_tx['DATE'].astype(str) >= last_time.strftime(FORMAT_DATE)) & (df_tx['DATE'].astype(str) <= first_time.strftime(FORMAT_DATE)) & (df_tx['HOUR'] == hour) ]\n",
    "      total = len(df_rec)\n",
    "      if total >= WEEKS_RECURRENT:\n",
    "        return df_tx[ (df_tx[\"USER\"] == p_user) ]\n",
    "    return pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processingRecurrentUsers(data_taxis):\n",
    "    start_time = time.perf_counter()\n",
    "    df_user = data_taxis.loc[:,['USER']].copy()\n",
    "    df_new = pd.DataFrame({})\n",
    "    total_user, removed_users, count, user_array = 0, 0, 0, []\n",
    "    list_users = df_user.value_counts().index.to_list()\n",
    "    print(f\"Users to process {len(list_users)}\")\n",
    "\n",
    "    for user in list_users:\n",
    "        count += 1\n",
    "        user_array.append(user[0])\n",
    "        if count % 10000 == 0 or count >= len(list_users):\n",
    "            list_result = []\n",
    "            args = [(user, data_taxis[ (data_taxis[\"USER\"] == user) ], DATE_START_TRAINIG) for user in user_array]\n",
    "            with Pool() as pool:\n",
    "                list_result = pool.starmap(recurrentUsers, args)\n",
    "            for df in list_result:\n",
    "                if len(df) > 0:\n",
    "                    total_user += 1\n",
    "                else:\n",
    "                    removed_users += 1\n",
    "                df_new = pd.concat([df_new, df])\n",
    "            user_array = []\n",
    "            finish_time = time.perf_counter()\n",
    "            print(f\"Processing first {count} users of {len(df_user.value_counts().index.to_list())-count}, total recurents users: {total_user}, removed users {removed_users} in {format(finish_time-start_time)} seconds\")\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {format(finish_time-start_time)} seconds, total recurents users: {total_user}, removed users {removed_users} total rows {len(df_new)}\")\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing recurrent users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxis = processingRecurrentUsers(df_taxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of services per week is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "s_taxis = df_taxis.groupby(['DAYOFWEEK','HOUR'])['DATE'].count()\n",
    "s_taxis.plot(ax=ax)\n",
    "_ = ax.set(\n",
    "    title=\"Number of services per week\",\n",
    "    xticks=[i * 24 for i in range(7)],\n",
    "    xticklabels=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thr\", \"Fri\", \"Sat\"],\n",
    "    xlabel=\"Day of week\",\n",
    "    ylabel=\"Amount of services\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of services per hour is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "s_taxis = df_taxis.groupby(['HOUR'])['DATE'].count()\n",
    "s_taxis.plot(ax=ax)\n",
    "_ = ax.set(\n",
    "    title=\"Number of services per hour\",\n",
    "    xticks=[i for i in range(24)],\n",
    "    xticklabels=[i for i in range(24)],\n",
    "    xlabel=\"Hour of day\",\n",
    "    ylabel=\"Amount of services\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA VISUALIZATION\n",
    "\n",
    "* Scatter plot of the variables\n",
    "* Graph of distribution of variables (Low distribution, Low prediction)\n",
    "* Correlation graphs (Heat map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se presenta un gráfico general de los datos\n",
    "sns.pairplot(df_taxis, height=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_taxisThe correlation matrix is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = df_taxis.loc[:,['DAYOFWEEK','HOUR']].corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between variables in the data are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr_matrix, annot=True, mask = mask, cmap=cmap )\n",
    "plt.title('Relationship between dataframe variables',fontsize=20,fontname='serif')\n",
    "plt.ylabel('Y', color='green', fontsize=25)\n",
    "plt.xlabel('X', color='Red', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model and predict with test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictModel(model, X_train, y_train, X_test):\n",
    "    #A continuación se entrena el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    #Se crean las predicciones para pruebas\n",
    "    y_pred = model.predict(X_test)\n",
    "    #Se crean las probabilidades para pruebas\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    return model, y_pred, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the K-neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def knnModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de K vecinos\n",
    "    model = KNeighborsClassifier(n_neighbors=2)#, metric='euclidean',leaf_size=1, p=1, weights='uniform')\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Multi-layer Perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlpModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de perceptrones multi capa\n",
    "    model = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(30,30, 30), random_state=1,learning_rate_init=0.001,max_iter=5000)\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de perceptrones multi capa\n",
    "    model = Perceptron(eta0=0.1, n_iter_no_change=10, random_state=1)\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lrModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de de regesión logística\n",
    "    model = LogisticRegression(random_state = 1)\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rfcModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de los arboles aleatorios\n",
    "    model = RandomForestClassifier(criterion='gini', max_depth=5, n_estimators=200, class_weight=\"balanced\")\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Support Vector Machine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svcModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de máquina de soporte de vectores\n",
    "    model = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0, class_weight='balanced')#, class_weight='balanced'\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dtcModel(X_train, y_train, X_test):\n",
    "    #Se aplica el modelo de los arboles de desiciones\n",
    "    model = DecisionTreeClassifier(criterion='gini', max_depth=3, class_weight='balanced')#class_weight={0: 1, 1: 5}\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def etcModel(X_train, y_train, X_test):\n",
    "    model = ExtraTreesClassifier(n_estimators=1, max_depth=1, min_samples_split=2, random_state=0, class_weight=\"balanced\")\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate Gradient Boosting Classifier, this function supports both binary and multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gbcModel(X_train, y_train, X_test):\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def abcModel(X_train, y_train, X_test):\n",
    "    model = AdaBoostClassifier(n_estimators=1,algorithm='SAMME',random_state=0)\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that evaluate the Stochastic Gradient Descent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sgdModel(X_train, y_train, X_test,):\n",
    "    #Se aplica el modelo de gradiente descendente estocástico\n",
    "    model =SGDClassifier(loss='huber', random_state=1, max_iter=2000, epsilon=0.1)#loss='hinge'\n",
    "    return predictModel(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to display the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusionMatrix(model, y_test, y_pred):\n",
    "  plt.rcParams.update({'font.size': 16})\n",
    "  cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=model.classes_)\n",
    "  disp.plot()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate a user's probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ProbabilisticClassifier(holiday, hour, dayOfWeek, data):\n",
    "  df_onset = data[ (data['HOLIDAY'] == holiday) & (data['DAYOFWEEK'] == dayOfWeek) & (data['HOUR'] == hour) & (data['SERVICE'] == 1) ]\n",
    "  total_onset = len(df_onset)\n",
    "  if total_onset > 0:\n",
    "    return total_onset / TRAIN_WEEK\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate the probabilistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def probabilisticModel(data, date_predict):\n",
    "    co_holidays = holidays.CO()\n",
    "    threshold = 0.7\n",
    "    array_result = []\n",
    "    array_prob = []\n",
    "    for hour in range(HOURS_OF_DAY):\n",
    "        result = ProbabilisticClassifier(1 if co_holidays.get(date_predict) != None else 0, hour, date_predict.isoweekday(), data)\n",
    "        array_prob.append(round(result,2))\n",
    "        array_result.append(1 if result >= threshold else 0)\n",
    "    return array_result, array_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to validate if it contains all data in 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def existAllZero(arr):\n",
    "    if len(arr)>0:\n",
    "        for value in arr:\n",
    "            if value != 0:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the name of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelName(model_type):\n",
    "    if model_type == 'LR':\n",
    "        return 'Logistic regression'\n",
    "    elif model_type == 'KNN':\n",
    "        return 'Kneighbors'\n",
    "    elif model_type == 'MLP':\n",
    "        return 'Multilayer perceptron'\n",
    "    elif model_type == 'RFC':\n",
    "        return 'Random forest'\n",
    "    elif model_type == 'SVC':\n",
    "        return 'Support vector'\n",
    "    elif model_type == 'SGD':\n",
    "        return 'Gradient descent stochastic'\n",
    "    elif model_type == 'DTC':\n",
    "        return 'Decision tree'\n",
    "    elif model_type == 'GBC':\n",
    "        return 'Gradient boosting'\n",
    "    elif model_type == 'ETC':\n",
    "        return 'Extra trees'\n",
    "    elif model_type == 'ABC':\n",
    "        return 'Ada Boost'\n",
    "    elif model_type == 'PR':\n",
    "        return 'Probabilistic'\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Function to fill the dataset with the services taken in 1 and those not taken in 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setDataService(data, from_date, to_date):\n",
    "  co_holidays = holidays.CO()\n",
    "  remaining_days = to_date.date() - from_date.date()\n",
    "  service_array = []\n",
    "  existService = 0\n",
    "  for i in range(remaining_days.days+1):\n",
    "    last_time = from_date + timedelta(days=i)\n",
    "    if last_time.date().isoweekday() == to_date.date().isoweekday():\n",
    "      for hour in range(HOURS_OF_DAY):\n",
    "        #Se filtra por el mismo día de la semana, fecha y hora\n",
    "        #df_service = data[(data['DIADESEMANA'] == last_time.isoweekday()) & (data['FECHA'] == last_time.date()) & (data['HORA'] == hour) ].to_numpy()\n",
    "        df_service = data[ (data['DATE'].astype(str) == last_time.strftime(FORMAT_DATE)) & (data['HOUR'] == hour) ]\n",
    "        if len(df_service) > 0:\n",
    "          #print(f\"fecha con servicio ={last_time.strftime(FORMAT_DATE)}, hour ={hour}\")\n",
    "          #Se llena con valor en 1 porqué se encontró un servicio\n",
    "          existService = 1\n",
    "        else:\n",
    "          #print(f\"fecha sin servicio ={last_time.strftime(FORMAT_DATE)}, hour ={hour}\")\n",
    "          #Se llena con valor en 0 porqué no se encontró ningún servicio\n",
    "          existService = 0        \n",
    "        df_service = [1 if co_holidays.get(last_time) != None else 0,last_time.date().strftime(FORMAT_DATE),last_time.isoweekday(),hour,existService]\n",
    "        service_array.append(df_service)\n",
    "  return pd.DataFrame(service_array, \n",
    "             columns=['HOLIDAY','DATE','DAYOFWEEK','HOUR','SERVICE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to test the different models in a configured time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function that trains the model and returns the y_predicted\n",
    "def evaluate_model(user, data, date_predict, model_type):\n",
    "  #Vector initialization\n",
    "  y_pred = np.empty(0,dtype=int)\n",
    "  y_prob = np.empty(0,dtype=float)\n",
    "  y_test = np.empty(0,dtype=int)\n",
    "  total_services = 0\n",
    "\n",
    "  y_result_arr = []\n",
    "  y_prob_arr = []\n",
    "  #Filter by user\n",
    "  data = data[data[\"USER\"]==user].copy()\n",
    "  #The telephone is deleted because it is not relevant to the model\n",
    "  data = data.drop(labels=['USER'], axis=1)\n",
    "\n",
    "  #The training start date is obtained\n",
    "  date_first_training = date_predict - timedelta(weeks=TRAIN_WEEK)\n",
    "  #The dataframe is partitioned from the start date to the end date of training\n",
    "  data = data[ (pd.to_datetime(data['DATE'], format=FORMAT_DATE) >= date_first_training) & (pd.to_datetime(data['DATE'], format=FORMAT_DATE) <= date_predict) ]\n",
    "\n",
    "  #The dataset is filled with the hours of the services that were not taken by the user\n",
    "  data = setDataService(data, date_first_training, date_predict)\n",
    "  #Create the training dataframe from the start date to the prediction date\n",
    "  train = data[ (pd.to_datetime(data['DATE'], format=FORMAT_DATE) >= date_first_training) & (pd.to_datetime(data['DATE'], format=FORMAT_DATE) < date_predict) ]\n",
    "  #Create the test dataframe from the prediction date up to one week ahead\n",
    "  test = data[(pd.to_datetime(data['DATE'], format=FORMAT_DATE) == date_predict) ]\n",
    "\n",
    "  #Data for testing the models\n",
    "  train = train.loc[:,['HOLIDAY','DAYOFWEEK','HOUR','SERVICE']]\n",
    "  test = test.loc[:,['HOLIDAY','DAYOFWEEK','HOUR','SERVICE']]\n",
    "\n",
    "  y_train = train.SERVICE\n",
    "  X_train = train.drop(labels='SERVICE', axis=1)\n",
    "\n",
    "  y_test = test.SERVICE\n",
    "  X_test = test.drop(labels='SERVICE', axis=1)\n",
    "\n",
    "  #Validated if you have more than one of the recurring services registered\n",
    "  total_services = len(train[ (train[\"SERVICE\"] == 1)])\n",
    "  if total_services >= WEEKS_RECURRENT:\n",
    "    if model_type != 'PR':\n",
    "      '''\n",
    "      scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "      #scaler = StandardScaler()\n",
    "      #scaler = MinMaxScaler(feature_range=(0,1))\n",
    "      #scaler = Normalizer()\n",
    "      scaler.fit(X_train)\n",
    "      X_train = scaler.transform(X_train)\n",
    "      scaler.fit(X_test)\n",
    "      X_test = scaler.transform(X_test)\n",
    "      '''\n",
    "    \n",
    "    if model_type == 'LR':\n",
    "      model, y_pred, y_prob = lrModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'KNN':\n",
    "      model, y_pred, y_prob = knnModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'MLP':\n",
    "      model, y_pred, y_prob = mlpModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'RFC':\n",
    "      model, y_pred, y_prob = rfcModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'SVC':\n",
    "      model, y_pred, y_prob = svcModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'SGD':\n",
    "      model, y_pred, y_prob = sgdModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'DTC':\n",
    "      model, y_pred, y_prob = dtcModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'GBC':\n",
    "      model, y_pred, y_prob = gbcModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'ETC':\n",
    "      model, y_pred, y_prob = etcModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'ABC':\n",
    "      model, y_pred, y_prob = abcModel(X_train, y_train, X_test)\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    elif model_type == 'PR':\n",
    "      #Se caulcula el modelo probabilistico\n",
    "      y_result, y_proba = probabilisticModel(train, date_predict)\n",
    "      \n",
    "      y_result_arr.append(y_result)\n",
    "      y_prob_arr.append(y_proba)\n",
    "      \n",
    "      for i in y_result_arr:\n",
    "        for j in i:\n",
    "          y_pred = np.append(y_pred, int(j))\n",
    "      for i in y_prob_arr:\n",
    "        for j in i:\n",
    "          y_prob = np.append(y_prob, float(j))\n",
    "      class model:\n",
    "        classes_ = np.empty(0,dtype=int)\n",
    "        name = 'Probabilistic'\n",
    "      model.classes_ = np.array([0, 1])\n",
    "      #reportClassification(model, X_train, y_train, y_pred, X_test, y_test, model_type)\n",
    "    else:\n",
    "      print(f\"Model {model_type} does not exist!\")\n",
    "  #else:\n",
    "  #  print(f\"Not enough services for date:{date_predict}, {total_services} services of user:{user}!\")\n",
    "  return y_pred, y_test, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with all models for one week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Models are removed due to poor performance: 'SVC','ETC','DTC','LR','SGD'\n",
    "models_array = ['PR','KNN','GBC','RFC','ABC','MLP']\n",
    "#models_array = ['PR']\n",
    "report_models = []\n",
    "roc_curve_arr = []\n",
    "user_array = []\n",
    "start_time = time.perf_counter()\n",
    "start_proccess = datetime.now()\n",
    "list_users = df_taxis.loc[:,['USER']].value_counts().index.to_list()\n",
    "print(f\"Start process: {datetime.now()} - {len(list_users)} users.\")\n",
    "for model in models_array:\n",
    "    start_date = datetime.now()\n",
    "    count_model = 0\n",
    "    kappa_score = 0.0\n",
    "    test_score = 0\n",
    "    roc_score = 0.0\n",
    "    count_users = 0\n",
    "    confusion_phone = np.matrix(0,dtype=int)\n",
    "    y_test_arr = np.array(0,dtype=int)\n",
    "    y_prob_arr = np.array(0,dtype=float)\n",
    "    for user in list_users:\n",
    "        count_users += 1\n",
    "        user_array.append(user[0])\n",
    "        if count_users % ( len(list_users) if len(list_users) < 1000 else 1000 ) == 0:\n",
    "            list_result = []\n",
    "            remaining_days = DATE_START_TRAINIG.date() + timedelta(weeks=TEST_WEEK) - DATE_START_TRAINIG.date()\n",
    "            for i in range(remaining_days.days):\n",
    "                last_time = DATE_START_TRAINIG.date() + timedelta(days=i)\n",
    "                args = [(user_a, df_taxis[ (df_taxis[\"USER\"] == user_a) ], pd.to_datetime(last_time, format=FORMAT_DATE), model) for user_a in user_array]\n",
    "                with Pool() as pool:\n",
    "                    list_result = pool.starmap(evaluate_model, args)\n",
    "                for result in list_result:\n",
    "                    y_pred = result[0]\n",
    "                    y_test = result[1]\n",
    "                    y_prob = result[2]\n",
    "                    if len(y_pred) > 1:\n",
    "                        if len(y_prob)>0:\n",
    "                            y_test = y_test.values[:]\n",
    "                            y_prob = [y_prob[:, 1] if model != 'PR' else y_prob[:]]\n",
    "                            y_test_arr = np.concatenate((y_test_arr, y_test), axis=None)\n",
    "                            y_prob_arr = np.concatenate((y_prob_arr, y_prob), axis=None)\n",
    "                        try:\n",
    "                            matrix = np.matrix(confusion_matrix(y_test, y_pred))\n",
    "                            if matrix.size == 1:\n",
    "                                matrix = np.matrix([[24,0],[0,0]],dtype=int)\n",
    "                            confusion_phone = confusion_phone + matrix\n",
    "                        except Exception as err:\n",
    "                            #print(f\"Error in confusion_matrix: {err}\")\n",
    "                            confusion_phone = confusion_phone\n",
    "                        try:\n",
    "                            score_k = cohen_kappa_score(y_test, y_pred)\n",
    "                            kappa_score += (0.0 if math.isnan (score_k) else score_k)\n",
    "                        except Exception as err:\n",
    "                            #print(f\"Error in kappa_score: {err}\")\n",
    "                            kappa_score += 0\n",
    "                        test_score += np.mean(y_pred == y_test)\n",
    "                        try:\n",
    "                            roc_score += roc_auc_score(y_test, y_pred)\n",
    "                        except Exception as err:\n",
    "                            #print(f\"Error in roc_score: {err}\")\n",
    "                            roc_score += 0.0\n",
    "                        count_model = count_model + 1\n",
    "                        #else:\n",
    "                        #    print(f\"The user: {phone} didn't take services on {last_time}\")\n",
    "                finish_time = time.perf_counter()\n",
    "                print(f\"Processing first {count_users} users of {len(list_users)-count_users} day {last_time}, in {format(finish_time-start_time)} seconds\")\n",
    "            user_array = []\n",
    "    roc_curve_arr.append([model, y_test_arr, y_prob_arr])\n",
    "    report_models.append([model,confusion_phone, (kappa_score/count_model) if kappa_score > 0 else 0, (test_score/count_model) if test_score > 0 else 0, (roc_score/count_model) if roc_score > 0 else 0])\n",
    "    print(f\"Processed {modelName(model)} model in: {datetime.now()-start_date}\")\n",
    "print(f\"Finish process: {datetime.now()} in: {datetime.now()-start_proccess}\")\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Se ejecuta la matriz de confusión de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_array = []\n",
    "for model in report_models:\n",
    "    true_neg = model[1][0,0]\n",
    "    true_pos = model[1][1,1]\n",
    "    false_pos = model[1][0,1]\n",
    "    false_neg = model[1][1,0]\n",
    "    kappa = model[2]\n",
    "    test_score = model[3]\n",
    "    roc = model[4]\n",
    "\n",
    "    #Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    precision_scr = true_pos / (true_pos + false_pos) if true_pos > 0 else 0\n",
    "    #Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "    recall_scr = true_pos / (true_pos + false_neg) if true_pos > 0 else 0\n",
    "    #F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    f1_scr = (2 * precision_scr * recall_scr) / (precision_scr + recall_scr) if true_pos > 0 else 0\n",
    "\n",
    "    #Sacar la tasa de los TP = Que es Positivos del test sobre los positivos predichos\n",
    "    #TP rate = TP / Positivos reales (test)\n",
    "    #FP rate = FP / Negativos reales (test)\n",
    "    models_array.append([modelName(model[0]),\n",
    "                         round(precision_scr,2),\n",
    "                         round(recall_scr,2),\n",
    "                         round(f1_scr,2),\n",
    "                         true_neg,\n",
    "                         true_pos,\n",
    "                         false_pos,\n",
    "                         false_neg,\n",
    "                         round(kappa,2),\n",
    "                         round(test_score,2),\n",
    "                         round(roc,2)])\n",
    "\n",
    "#Mostramos el dataframe con el reporte\n",
    "pd.DataFrame(models_array, columns = ['MODEL','PRECISION','RECALL','F1','TN','TP','FP','FN','KAPPA','SCORE TEST','ROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se muestra la curva ROC de los modelos\n",
    "for roc in roc_curve_arr:\n",
    "    if not existAllZero(roc[1]):\n",
    "        ns_probs = [0 for _ in range(len(roc[1]))]\n",
    "        # calculate scores\n",
    "        ns_auc = roc_auc_score(roc[1], ns_probs)\n",
    "        lr_auc = roc_auc_score(roc[1], roc[2])\n",
    "        # summarize scores\n",
    "        print(f'{modelName(roc[0])}: ROC AUC=%.3f' % (lr_auc))\n",
    "        # calculate roc curves\n",
    "        ns_fpr, ns_tpr, _ = roc_curve(roc[1], ns_probs)\n",
    "        lr_fpr, lr_tpr, _ = roc_curve(roc[1], roc[2])\n",
    "        # plot the roc curve for the model\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No model')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.', label=modelName(roc[0]))\n",
    "        # axis labels\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        # show the legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Model {modelName(roc[0])} has no values to display\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Proyecto predicción servicio taxi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c66cf74b0d2a9f1f4e9764d44e7ed6418e0349c942d329ab910649654051b15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
